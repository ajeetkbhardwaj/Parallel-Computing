{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96t9FLKcBCcz"
   },
   "source": [
    "<h1 align=\"center\">Title</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ajeetkbhardwaj/full-stack-project-portfolio/blob/main/DataScience/DS1-Hausing-Rental-Analysis-and-Prediction-In-Delhi.ipynb)  \n",
    "\n",
    "\n",
    "[![Open In Kaggle](https://en.wikipedia.org/wiki/Kaggle#/media/File:Kaggle_Logo.svg)](https://www.kaggle.com/code/ajeetkumarbhardwaj/notebook6605809d8c/edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mitRSbmUBj2y"
   },
   "source": [
    "1. **What is CPU ?**\n",
    "\n",
    "[](https://media.geeksforgeeks.org/wp-content/uploads/20250719153951741828/CPU-Components-.webp)\n",
    "\n",
    "2. What is GPU ?\n",
    "\n",
    "\n",
    "Difference between difference processing techniques ?\n",
    "\n",
    "CPU, GPU, NPU and TPU - The Real Differences for AI/ML\n",
    "\n",
    "CPU (Central Processing Unit)\n",
    "The classic processor in every computer. CPUs can run any software, including AI models, but are slower for deep learning due to fewer parallel cores.\n",
    "Best for:\n",
    "- Traditional machine learning (scikit-learn, XGBoost)\n",
    "- Running small models or prototypes\n",
    "- General-purpose tasks and light inference\n",
    "\n",
    "GPU (Graphics Processing Unit)\n",
    "GPUs are built for parallel processing. They are the backbone of modern deep learning, perfect for training and inference of models like CNNs, RNNs, and transformers (GPT, BERT, ResNet).\n",
    "Best for:\n",
    "- Training and running large deep learning models\n",
    "- Supported by all major AI libraries\n",
    "- Flexible for many AI workloads\n",
    "\n",
    "NPU (Neural Processing Unit)\n",
    "NPUs are specialised chips designed only for neural network operations, often embedded in smartphones and IoT devices. They run efficient models for vision, speech, and edge AI.\n",
    "Best for:\n",
    "- On-device, real-time AI (face unlock, language translation)\n",
    "- Battery-friendly AI in mobile and IoT\n",
    "- Lightweight, efficient models\n",
    "\n",
    "TPU (Tensor Processing Unit)\n",
    "TPUs are Google’s custom AI accelerators, tuned for TensorFlow and massive neural networks. Ideal for training and deploying large models at cloud scale.\n",
    "Best for:\n",
    "- Scalable deep learning in Google Cloud\n",
    "- Training and inference for big models (BERT, GPT-2, EfficientNet)\n",
    "- High-speed tensor calculations\n",
    "\n",
    "Which AI models run on each?\n",
    "CPU: Any model, but best for classical ML, prototyping, and small-scale inference\n",
    "GPU: All deep learning models (CNNs, RNNs, transformers)\n",
    "NPU: Optimised mobile and edge models (MobileNet, tiny BERT)\n",
    "TPU: Large-scale neural networks in TensorFlow\n",
    "\n",
    "Note DPUs (Data Processing Units):\n",
    "DPUs don’t run AI models directly, but they play a key role in modern AI infrastructure. They accelerate data movement, networking, and storage, freeing up CPUs and GPUs for computation making large-scale AI systems faster and more efficient.\n",
    "\n",
    "3. What is Parallel Processing ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LeTtt7MjCq3"
   },
   "source": [
    "What is parallel programming on GPU ?\n",
    "> Parallel transfer of data from CPU to GPU for computation  using Cuda(C/C++) programming language\n",
    "\n",
    "**Story of School Team Work to Parallel Computing** :\n",
    "Everyone have attended the school from their childhood to the Pre-College, Therefore Let's Imagine a school with several classrooms. Each classroom is different—some bigger, some smaller, depending on what the school can afford, but every classroom is filled with students waiting for assignments.\n",
    "\n",
    "Now, there's a large pile of tasks that needs to be handled. Rule number one: no single classroom is allowed more than 1,024 tasks at any time. The work in each classroom happens differently, too—not every student tackles their own task immediately on their own.\n",
    "\n",
    "Inside each classroom, tasks are handed out in special groups known as warps. Each warp is like a tight-knit team of students—32 of them move in perfect coordination, all working on their assigned tasks together, step by step without falling out of lockstep (everyone on the team does the same thing at the same time, or they wait for those who aren't done yet).\n",
    "\n",
    "At any moment, within a classroom, the number of tasks being actively worked on equals 32 times the number of warps that classroom can support. If, for instance, every classroom has 4 warps, then 128 (32 x 4) tasks are being processed at once in that classroom. Multiply that by the number of classrooms you have in the school: for N classrooms, a total of 32 x number of warps x N tasks are being worked on at the same moment.\n",
    "\n",
    "The team size in each classroom (the number of warps) depends on how fancy or advanced your school is—that is, what kind of computer you have. So as more classrooms join, and each classroom handles more warps, the whole school can get through that mountain of tasks much, much faster.\n",
    "\n",
    "\n",
    "What we have learn from this story about the parallel processing on GPU devices ?\n",
    "\n",
    "1. The \"school\" represents a GPU device.\n",
    "2. Each \"classroom\" is a CUDA thread block.\n",
    "3. The number of classrooms is variable, like the number of thread blocks being launched depending on work.\n",
    "4. Each classroom (thread block) can handle a maximum of 1024 tasks (threads in a block).\n",
    "5. Students in classrooms correspond to individual CUDA threads.\n",
    "6. Within each classroom, tasks are executed in groups called \"warps.\"\n",
    "7. A warp consists of 32 threads that execute instructions together simultaneously in lockstep.\n",
    "8. The number of warps per classroom depends on GPU architecture.\n",
    "9. At a given time, the number of tasks running = 32 (warp size) x number of warps x number of classrooms.\n",
    "10. This structure allows parallel processing of many tasks simultaneously, speeding up completion.\n",
    "11. The analogy highlights how CUDA organizes parallel execution with hierarchical thread grouping: grids of blocks, blocks of threads, threads in warps.\n",
    "12. Workload is divided efficiently among threads, with synchronization inside blocks (classrooms) but not between them.\n",
    "13. This provides scalability as more blocks can be launched on multiple GPU multiprocessors.\n",
    "\n",
    "\n",
    "**Physical** : These are observable and have fixed quantity such as students.\n",
    "1. Streaming Processors(SPs/Cores) : They are main processing unit on GPU and capable of executing computation concurrently on multiple data elements. Thus, SPs(Cores) as Individual Students and more Cores means the greater the number of tasks that can be processed concurrently.\n",
    "\n",
    "2. Streaming Multi-Processor(SM/multi-processor) : a collection/group of SPs(Cores) like a classroom that accommodates multiple student(SPs/Core). Therefore, SM acts as a higher-level unit that manages and coordinates the execution of tasks across the SPs within it.\n",
    "\n",
    "Note :\n",
    "1. SM = classroom (manages execution, resources, scheduling)\n",
    "\n",
    "2. Core (SP) = student (executes individual tasks/threads)\n",
    "\n",
    "The number of Streaming Multiprocessors (SMs) and Streaming Processors (SPs or cores) varies by GPU model and architecture, with SMs being higher-level units that manage groups of cores, and the total counts are fixed for a given GPU design. For example, a GPU might have 16 SMs, each containing 32 or more cores, but the exact numbers depend on the specific NVIDIA GPU model.\n",
    "\n",
    "\n",
    "\n",
    "**Logical** : These are't directly observable but can be imagined or conceptualized with an unspecified quantity such as tasks.\n",
    "\n",
    "What are the CUDA threads, blocks and grids ?\n",
    "\n",
    "A thread in CUDA represents a single unit of work or task, just like one job that needs to be done. It is the smallest execution unit, and each thread runs independently.\n",
    "\n",
    "A block is a group or collection of threads. These threads in a block work together and can share data via fast shared memory. The block size (number of threads in a block) is limited by hardware — typically up to 1024 threads per block. This is because all threads in a block execute on the same Streaming Multiprocessor (SM) and share its limited resources.\n",
    "\n",
    "The grid is a larger structure made up of multiple blocks. Blocks within a grid perform the same kernel function but work on different parts of the data or tasks. The grid size can vary depending on the total workload.\n",
    "\n",
    "Using your example: If there are 6 blocks and each block contains 12 threads, then the whole grid has\n",
    "6\n",
    "×\n",
    "12\n",
    "=\n",
    "72\n",
    "6×12=72 threads executing in parallel.\n",
    "\n",
    "Threads and blocks are indexed to identify their position, similar to a matrix coordinate system:\n",
    "\n",
    "blockIdx identifies which block in the grid the thread belongs to.\n",
    "\n",
    "threadIdx identifies the thread within its block.\n",
    "These indices help a thread know exactly what data to work on.\n",
    "\n",
    "We divide threads into blocks because of two main rules:\n",
    "\n",
    "Maximum Threads per Block Limit: Each block can have a maximum of (usually) 1024 threads to avoid resource contention on an SM.\n",
    "\n",
    "Warp Execution Limits: Threads execute in groups of 32 called warps. If you put all 1024 threads in a single block, only 32 threads (one warp) execute simultaneously, and others wait, resulting in sequential execution in chunks. Splitting threads into multiple blocks allows many warps to run concurrently on different SMs, maximizing parallelism.\n",
    "\n",
    "The cake-eating analogy:\n",
    "\n",
    "Eating 32 cakes sequentially (one warp) inside a single block takes longer.\n",
    "\n",
    "Dividing 1024 cakes across 32 blocks of 32 threads lets you eat all 1024 cakes at once in parallel, speeding up the task dramatically.\n",
    "\n",
    "Thus, Thead is one single task/job, block is a group of threads working togathe, limited to usually 1024 threads and grid is a collection of blocks doing the same kernel function over the full workload.\n",
    "\n",
    "Threads and blocks have indices to identify their work division.\n",
    "\n",
    "Dividing threads into smaller blocks avoids hardware limits and enables more efficient parallel execution through multiple warps running concurrently.\n",
    "\n",
    "This hierarchical structure (thread → block → grid) underlies CUDA’s ability to perform massive parallel computation effectively.\n",
    "\n",
    "In short, warps are the execution units sending groups of 32 threads through instructions together on the GPU hardware, while blocks are logical groups of threads that share resources like shared memory and coordinate with synchronization to collaboratively solve parts of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iFnQEHJE0za"
   },
   "source": [
    "What is Cuda ?\n",
    "> CUDA (Compute Unified Device Architecture) is a **parallel computing platform and programming model** developed by NVIDIA that allows developers to leverage the massive parallel processing power of NVIDIA GPUs for tasks beyond graphics rendering, such as scientific computing, machine learning, and data analysis.\n",
    "\n",
    "\n",
    "How CUDA works ?\n",
    "- **Programming Model:** Developers write CUDA code in C, C++, or other supported languages, using special CUDA extensions. CUDA code typically contains two main components:\n",
    "  - **Host code** runs on the CPU.\n",
    "  - **Device code (kernels)** runs on the GPU.\n",
    "\n",
    "- **Compilation Process:**\n",
    "  - CUDA code is written in files with a `.cu` extension.\n",
    "  - NVIDIA provides a specialized compiler called **NVCC (NVIDIA CUDA Compiler)**. NVCC analyzes the source and separates CPU (host) and GPU (device) code[3].\n",
    "    - The **host code** is compiled with a standard compiler like GCC or MSVC.\n",
    "    - The **device code** is compiled and optimized for NVIDIA GPUs, first into an intermediate language called **PTX (Parallel Thread Execution)**, then further translated into GPU-specific machine code (SASS) by the NVIDIA driver[3].\n",
    "  - Finally, the host and device components are linked together into an executable that can launch GPU kernels[3].\n",
    "\n",
    "- **Execution Model:**\n",
    "  - On runtime, the CPU (host) part of the program launches one or more **kernels** (GPU functions) to execute on the device (GPU).\n",
    "  - CUDA organizes parallel execution in a hierarchy:\n",
    "    - A **kernel** is launched as a *grid* of *blocks*.\n",
    "    - Each block contains many *threads*, so a single launch can run thousands to millions of threads concurrently[11].\n",
    "    - This mapping leverages the **thousands of CUDA cores** present in modern NVIDIA GPUs, achieving massive parallelism[12].\n",
    "  - Each thread executes the same code but can process different data (SIMT model—Single Instruction, Multiple Threads)[1][11].\n",
    "\n",
    "- **Memory Model:**\n",
    "  - CUDA exposes multiple memory spaces:\n",
    "    - **Global memory** (large but slow, accessible by all threads)\n",
    "    - **Shared memory** (fast, within each block)\n",
    "    - **Local memory** (private to each thread)\n",
    "    - **Constant and texture memory** (read-only spaces with specific optimizations)[3]\n",
    "  - Effective memory use and minimizing data transfer between CPU and GPU are key to high performance[3].\n",
    "\n",
    "- **Workflow Overview:**\n",
    "  1. **Write CUDA code (.cu):** Mark kernel functions with `__global__`, device-only functions with `__device__`, and CPU functions with standard C/C++.\n",
    "  2. **Compile:** Use NVCC to build the program and generate a host CPU binary plus GPU code[3].\n",
    "  3. **Manage memory:** Allocate and transfer data between CPU and GPU memory as needed[3].\n",
    "  4. **Launch kernels:** Use standard CUDA runtime APIs to launch GPU tasks from the CPU host application[3].\n",
    "  5. **Retrieve results:** Copy results from GPU memory back to CPU memory after computation completes[3].\n",
    "\n",
    "Why CUDA Is Effective\n",
    "\n",
    "- **Massive parallelism:** The GPU can execute tens of thousands of lightweight threads, far exceeding the parallelism available on CPUs.\n",
    "- **Efficiency:** Offloading compute-intensive parts of applications to the GPU drastically speeds up many workloads, especially those that can be broken into independent tasks[1][4][6].\n",
    "- **Ecosystem:** CUDA is supported by a wide range of libraries, tools, and high-level frameworks, making it accessible for various applications and research fields[1][3][6].\n",
    "\n",
    "How to transfer data from cpu to gpu and utilize the gpu core for computation ?\n",
    ">  **Transferring data from the CPU to the GPU** and utilizing GPU cores in CUDA involves several steps, due to the fact that the CPU and GPU have **separate physical memory spaces**. Data must be explicitly copied between these spaces, typically via the PCIe bus.\n",
    "\n",
    "GPU functioning and components ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTE5h9LhI7WD"
   },
   "source": [
    "Basic Workflow\n",
    "\n",
    "1. **Allocate memory on the GPU:**  \n",
    "   Use `cudaMalloc()` to allocate device (GPU) memory for inputs and outputs.\n",
    "\n",
    "2. **Copy data from CPU to GPU:**  \n",
    "   Use `cudaMemcpy()` (with the `cudaMemcpyHostToDevice` flag) to copy data from CPU (host) memory to the GPU (device) memory[8].\n",
    "   ```c\n",
    "   cudaMalloc((void**)&d_data, size);\n",
    "   cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);\n",
    "   ```\n",
    "   - Because CPU and GPU cannot directly access each other’s memory, this explicit copy is required[1][8].\n",
    "\n",
    "3. **Launch the kernel (utilize GPU cores):**\n",
    "   Write a kernel function (marked with `__global__`) to run on the GPU. Launch it with a specified number of blocks and threads, which determines how many GPU cores are used.\n",
    "   ```c\n",
    "   myKernel>>(d_data);\n",
    "   ```\n",
    "   - Each thread executes on a separate CUDA core, enabling massive parallelism[1][8].\n",
    "\n",
    "4. **Copy results back from GPU to CPU:**  \n",
    "   After computation, use `cudaMemcpy()` (with the `cudaMemcpyDeviceToHost` flag) to transfer results from GPU memory back to CPU memory[8].\n",
    "   ```c\n",
    "   cudaMemcpy(h_result, d_result, size, cudaMemcpyDeviceToHost);\n",
    "   ```\n",
    "\n",
    "5. **Free GPU memory:**  \n",
    "   Always free GPU memory with `cudaFree()` when done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yse-VSlkJMoh"
   },
   "source": [
    "### Step-1\n",
    "NVCC Jupyter Extension - used to run the c and cpp code on the notebook cells, via annotation of the each cell with `%%cuda` at the top.\n",
    "\n",
    "Note : Syntax checking may mean a lot of read and yellow lines on our code, since it's focused on Python code, so it's best to turn this feature off: Settings -> Editor -> Code diagnostics -> None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3486,
     "status": "ok",
     "timestamp": 1753188100980,
     "user": {
      "displayName": "Ajeet Kumar",
      "userId": "04620708898093466225"
     },
     "user_tz": -330
    },
    "id": "3fEQtE1SBd0N",
    "outputId": "1cd2bb3e-efdf-4706-f062-c77453bc2527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n",
      "Detected platform \"Colab\". Running its setup...\n",
      "Source files will be saved in \"/tmp/tmptc5etxjt\".\n"
     ]
    }
   ],
   "source": [
    "# check the Nvidia CUDA compiler driver install or not on T4 GPU of colab\n",
    "!nvcc --version\n",
    "# installing necessary package for running cuda kernel on the colab gpu in notebook\n",
    "!pip install nvcc4jupyter --quiet\n",
    "\n",
    "# loading the package extension\n",
    "%load_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1749,
     "status": "ok",
     "timestamp": 1753188340190,
     "user": {
      "displayName": "Ajeet Kumar",
      "userId": "04620708898093466225"
     },
     "user_tz": -330
    },
    "id": "SkfkFYUlJXSC",
    "outputId": "a468af79-c200-4be3-903b-b8e1b3755012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from the CPU.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include<stdio.h>\n",
    "void hello()\n",
    "{\n",
    "  printf(\"Hello from the CPU.\\n\");\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  hello();\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1753188741799,
     "user": {
      "displayName": "Ajeet Kumar",
      "userId": "04620708898093466225"
     },
     "user_tz": -330
    },
    "id": "KZjVYJRLKxnb",
    "outputId": "e8ae9ec0-67c3-4f14-e74e-93abe85f4b95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void helloGPU()\n",
    "{\n",
    "  printf(\"Hello from the GPU.\\n\");\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  helloGPU<<<1, 1>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAO1qRg9L1Er"
   },
   "source": [
    "In CUDA programming, **blocks and threads** are the fundamental components used to organize parallel work on the GPU. The triple angle bracket syntax:\n",
    "\n",
    "```cpp\n",
    "kernel<<<NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>(...);\n",
    "```\n",
    "specifies how many blocks and threads per block to launch for a kernel.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Thread:** The smallest unit of execution in CUDA. Each thread runs an instance of the kernel function.\n",
    "- **Block:** A group of threads that execute together on the same Streaming Multiprocessor (SM). Threads in a block can cooperate by sharing data through **shared memory** and can synchronize with each other.\n",
    "- **Grid:** A collection of blocks launched together for a kernel. All blocks in a grid run independently.\n",
    "\n",
    "### Execution Configuration\n",
    "\n",
    "- `>>` defines how many blocks form the grid and how many threads are in each block.\n",
    "- The total number of threads launched is:\n",
    "  \n",
    "  $$\n",
    "  \\text{total threads} = \\text{number_of_blocksks} \\times \\text{threads_perr\\_block}\n",
    "  $$\n",
    "\n",
    "### Examples\n",
    "\n",
    "| Kernel Call           | Meaning                                                               | Total Threads Launched |\n",
    "|----------------------|------------------------------------------------------------------------|-----------------------|\n",
    "| `kernelA>>()` | 1 block with 1 thread → executes kernel once                           | 1                     |\n",
    "| `kernelB>>()`| 1 block with 10 threads → kernel runs 10 times in parallel             | 10                    |\n",
    "| `kernelC>>()`| 10 blocks, 1 thread each → kernel runs 10 times                        | 10                    |\n",
    "| `kernelD>>()`| 10 blocks, 10 threads each → kernel runs 100 times                     | 100                   |\n",
    "\n",
    "### Hierarchy and Dimensions\n",
    "\n",
    "CUDA supports 1D, 2D, and 3D configurations for blocks and grids. For example:\n",
    "\n",
    "```cpp\n",
    "dim3 threadsPerBlock(16, 16);       // 256 threads per block in 2D\n",
    "dim3 numBlocks(N / 16, N / 16);     // 2D grid of blocks\n",
    "kernel>>(...);\n",
    "```\n",
    "\n",
    "- Using multidimensional blocks and grids helps in mapping naturally onto multidimensional data (e.g., matrices or volumes), but does not change the underlying execution model.\n",
    "\n",
    "### How Thread Indexing Works\n",
    "\n",
    "Within the kernel, each thread can access its own unique identifiers to know which data to work on:\n",
    "\n",
    "- `threadIdx.x` (and `.y`, `.z`) give the thread’s index within its block.\n",
    "- `blockIdx.x` (and `.y`, `.z`) give the block’s index within the grid.\n",
    "- `blockDim.x` tells how many threads are in each block along the x dimension.\n",
    "\n",
    "You usually calculate a **global thread index** like this (in 1D):\n",
    "\n",
    "```cpp\n",
    "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "```\n",
    "\n",
    "This index `idx` uniquely identifies the data element the thread should process.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- CUDA organizes parallel execution into **grids of blocks**, with each block having multiple **threads**.\n",
    "- The kernel launch syntax `>>` controls this hierarchy.\n",
    "- Threads in the same block can share memory and synchronize, which is key to efficient parallel algorithms.\n",
    "- The number of threads and blocks must be chosen to match your problem size and GPU architecture to maximize utilization.\n",
    "\n",
    "This hierarchical structure allows CUDA to scale from a few threads up to thousands or millions, fully leveraging the GPU’s massive parallelism[1][2][4][8][9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YpabT12MzQz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meLAA8BORfBq"
   },
   "source": [
    "## Books and Websites\n",
    "1. https://ncclux.github.io/NCC-Trainings/courses/\n",
    "2. https://github.com/karpathy/llm.c/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-jLI9RPCJ-h"
   },
   "source": [
    "## Rerences\n",
    "1. [geekforgeek-cpu](https://www.geeksforgeeks.org/computer-science-fundamentals/central-processing-unit-cpu/)\n",
    "2. [What is CPU, GPU and Parallel Processing ?](https://github.com/CisMine/Parallel-Computing-Cuda-C/blob/main/Chapter01/README.md)\n",
    "4. [How does computer works](https://artsandculture.google.com/story/how-computers-work/DgWBfSPf_sFSow?hl=en)\n",
    "5. [How does computer works - memory and processing](https://homepage.cs.uri.edu/faculty/wolfe/book/Readings/Reading04.htm)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMGHOnUYQM+G+KyXoSUkPpX",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
